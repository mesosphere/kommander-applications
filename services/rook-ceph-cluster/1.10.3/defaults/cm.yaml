apiVersion: v1
kind: ConfigMap
metadata:
  name: rook-ceph-cluster-1.10.3-d2iq-defaults
  namespace: ${releaseNamespace}
data:
  values.yaml: |
    ---
    operatorNamespace: ${releaseNamespace}
    clusterName: dkp-ceph-cluster
    toolbox:
      # If needed, enable a toolbox for debugging (creates a pod with ceph CLI)
      enabled: false

    # PSP is Unsupported in 1.25+ k8s
    # Set this to the same value as the rook-ceph chart.
    pspEnable: false

    # All values below are taken from the CephCluster CRD
    cephClusterSpec:
      cephVersion:
        image: quay.io/ceph/ceph:v17.2.3
      dataDirHostPath: /var/lib/rook

      mon:
        count: 3
        # Mons should only be allowed on the same node for test environments where data loss is acceptable.
        allowMultiplePerNode: false
        volumeClaimTemplate:
          spec:
            # Use the default storage class configured in cluster.
            # not setting storageClass to let it fall to environment default
            volumeMode: FileSystem
            resources:
              requests:
                storage: 10Gi

      mgr:
        count: 2
        allowMultiplePerNode: false

      dashboard:
        enabled: true
        urlPrefix: ""
        port: 8443
        ssl: false

      storage:
        storageClassDeviceSets:
          - name: rook-ceph-osd-set1
            count: 3
            portable: false
            encrypted: false
            placement:
              topologySpreadConstraints:
              - maxSkew: 1
                topologyKey: topology.kubernetes.io/zone # The nodes in the same rack have the same topology.kubernetes.io/zone label.
                whenUnsatisfiable: ScheduleAnyway
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - rook-ceph-osd
                        - rook-ceph-osd-prepare
              - maxSkew: 1
                topologyKey: kubernetes.io/hostname
                whenUnsatisfiable: ScheduleAnyway
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - rook-ceph-osd
                        - rook-ceph-osd-prepare
            volumeClaimTemplates:
              - metadata:
                  name: data
                spec:
                  resources:
                    requests:
                      storage: 20Gi
                  # Use the default storage class configured in cluster.
                  # not setting storageClass to let it fall to environment default
                  # OSD Requires Block storage.
                  volumeMode: Block
                  accessModes:
                    - ReadWriteOnce
        onlyApplyOSDPlacement: false
        useAllDevices: false
        useAllNodes: false

    ingress:
      # TODO(takirala): validate ingress is functioning correctly
      # TODO(takirala): create a docs ticket note on how to access dashboard (fetch secret, base64decode password etc.,)
      dashboard:
        enabled: true
        annotations:
          kubernetes.io/ingress.class: kommander-traefik
          ingress.kubernetes.io/auth-response-headers: X-Forwarded-User
          traefik.ingress.kubernetes.io/router.tls: "true"
          traefik.ingress.kubernetes.io/router.middlewares: "${releaseNamespace}-stripprefixes@kubernetescrd,${releaseNamespace}-forwardauth@kubernetescrd"
        host:
          name: ""
          path: "/dkp/kommander/ceph-dashboard"
        tls: []

    logLevel: DEBUG

    cephBlockPools: []
    cephFileSystems: []
    cephObjectStores:
      - name: dkp-object-store
        # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Object-Storage/ceph-object-store-crd.md#object-store-settings for available configuration
        spec:
          metadataPool:
            replicated:
              size: 1
          dataPool:
            replicated:
              size: 1
          preservePoolsOnDelete: false
          gateway:
            port: 80
            # securePort: 443
            instances: 1
            priorityClassName: system-cluster-critical
          healthCheck:
            bucket:
              interval: 60s
        storageClass:
          enabled: true
          name: dkp-object-store # Defined once per namespace

    monitoring:
      # TODO(takirala): enable monitoring
      enabled: false
      createPrometheusRules: false

    #################################################################
    ## BEGIN DKP specific config overrides                         ##
    ## This is added as a workaround to use the same configmap for ##
    ## both rook-ceph-cluster & object-bucket-claims helmreleases. ##
    #################################################################
    dkp:
      velero:
        enabled: true
        bucketName: dkp-velero
        storageClassName: dkp-object-store
        additionalConfig:
          maxSize: "10G"
      grafana-loki:
        enabled: true
        bucketName: dkp-loki
        storageClassName: dkp-object-store
        additionalConfig:
          # maxObjects: "1000"
          maxSize: "50G"
    #################################################################
    ## END of dkp specific config overrides                        ##
    #################################################################
